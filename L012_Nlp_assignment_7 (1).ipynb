{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**NAME: KIRTAN DAVE**\n",
        "\n",
        "**SAP ID: 40778230015**\n",
        "\n",
        "**ROLL NO: L012**\n",
        "\n",
        "**MSc DATA SCIENCE AND AI**"
      ],
      "metadata": {
        "id": "Y6ToB27YN6ep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Write a Python program using NLTK to extract named entities from the sentence: \"Apple Inc. is looking at buying U.K. startup for $1 billion.\""
      ],
      "metadata": {
        "id": "nEtLo_fJAffy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wwLSzot-u3g",
        "outputId": "75cf5792-a6ea-4a8b-9e9d-b79e4d821237"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PERSON Apple/NNP)\n",
            "  (ORGANIZATION Inc./NNP)\n",
            "  is/VBZ\n",
            "  looking/VBG\n",
            "  at/IN\n",
            "  buying/VBG\n",
            "  U.K./NNP\n",
            "  startup/NN\n",
            "  for/IN\n",
            "  $/$\n",
            "  1/CD\n",
            "  billion/CD\n",
            "  ./.)\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "nltk.download('words')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "sentence = \"Apple Inc. is looking at buying U.K. startup for $1 billion.\"\n",
        "tokens = word_tokenize(sentence)\n",
        "pos_tags = pos_tag(tokens)\n",
        "named_entities = ne_chunk(pos_tags)\n",
        "\n",
        "print(named_entities)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Using NLTK, write a function that takes a list of sentences and returns a list of named entities found in each sentence."
      ],
      "metadata": {
        "id": "6PEK7ajHA4nA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_named_entities(sentences):\n",
        "    named_entities_list = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        tokens = word_tokenize(sentence)\n",
        "        pos_tags = pos_tag(tokens)\n",
        "        named_entities = ne_chunk(pos_tags)\n",
        "        named_entities_list.append(named_entities)\n",
        "\n",
        "    return named_entities_list\n",
        "\n",
        "sentences = [\n",
        "    \"Apple Inc. is looking at buying U.K. startup for $1 billion.\",\n",
        "    \"Elon Musk founded SpaceX in 2002.\"\n",
        "]\n",
        "\n",
        "for ne in extract_named_entities(sentences):\n",
        "    print(ne)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaYFSwqhA8Eg",
        "outputId": "39abfd80-c60b-4055-d9c3-c89795a93146"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PERSON Apple/NNP)\n",
            "  (ORGANIZATION Inc./NNP)\n",
            "  is/VBZ\n",
            "  looking/VBG\n",
            "  at/IN\n",
            "  buying/VBG\n",
            "  U.K./NNP\n",
            "  startup/NN\n",
            "  for/IN\n",
            "  $/$\n",
            "  1/CD\n",
            "  billion/CD\n",
            "  ./.)\n",
            "(S\n",
            "  (PERSON Elon/NNP)\n",
            "  (PERSON Musk/NNP)\n",
            "  founded/VBD\n",
            "  (ORGANIZATION SpaceX/NNP)\n",
            "  in/IN\n",
            "  2002/CD\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Write a Python program that uses NLTK to extract and display all noun phrases from a given text."
      ],
      "metadata": {
        "id": "hrj3uix3BANA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.chunk import RegexpParser\n",
        "\n",
        "def extract_noun_phrases(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_tags = pos_tag(tokens)\n",
        "\n",
        "    grammar = \"NP: {<DT>?<JJ>*<NN>+}\"\n",
        "    chunk_parser = RegexpParser(grammar)\n",
        "    chunked_tree = chunk_parser.parse(pos_tags)\n",
        "\n",
        "    for subtree in chunked_tree.subtrees():\n",
        "        if subtree.label() == 'NP':\n",
        "            print(\" \".join(word for word, tag in subtree.leaves()))\n",
        "\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "extract_noun_phrases(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTTRFfvmBAeI",
        "outputId": "7bbd3f18-981e-405c-8f0f-97c8d671996c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The quick brown fox\n",
            "the lazy dog\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Using NLTK, write a program to perform chunking on the sentence:\n",
        "\"He reckons the current account deficit will narrow to only 8 billion.\" and display the chunked tree."
      ],
      "metadata": {
        "id": "zvijvjsqBF6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install svgling\n",
        "import svgling\n",
        "import nltk\n",
        "from nltk.tree import Tree\n",
        "from IPython.display import display # Import display\n",
        "\n",
        "sentence = \"He reckons the current account deficit will narrow to only 8 billion.\"\n",
        "tokens = word_tokenize(sentence)\n",
        "pos_tags = pos_tag(tokens)\n",
        "\n",
        "grammar = r\"\"\"\n",
        "    NP: {<DT>?<JJ>*<NN.*>+}\n",
        "    VP: {<VB.*><NP|PP|CLAUSE>+$}\n",
        "    PP: {<IN><NP>}\n",
        "\"\"\"\n",
        "chunk_parser = RegexpParser(grammar)\n",
        "chunked = chunk_parser.parse(pos_tags)\n",
        "\n",
        "print(chunked)\n",
        "display(chunked)  # Use display instead of chunked.draw()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "SaS8Y3R8BGlA",
        "outputId": "93e6b0e3-f19f-4b46-c3d8-caa6d53d3e17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  He/PRP\n",
            "  reckons/VBZ\n",
            "  (NP the/DT current/JJ account/NN deficit/NN)\n",
            "  will/MD\n",
            "  narrow/VB\n",
            "  to/TO\n",
            "  only/RB\n",
            "  8/CD\n",
            "  billion/CD\n",
            "  ./.)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tree('S', [('He', 'PRP'), ('reckons', 'VBZ'), Tree('NP', [('the', 'DT'), ('current', 'JJ'), ('account', 'NN'), ('deficit', 'NN')]), ('will', 'MD'), ('narrow', 'VB'), ('to', 'TO'), ('only', 'RB'), ('8', 'CD'), ('billion', 'CD'), ('.', '.')])"
            ],
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px\" version=\"1.1\" viewBox=\"0,0,688.0,168.0\" width=\"688px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"5.81395%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">He</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PRP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"2.90698%\" y1=\"20px\" y2=\"48px\" /><svg width=\"10.4651%\" x=\"5.81395%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">reckons</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBZ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"11.0465%\" y1=\"20px\" y2=\"48px\" /><svg width=\"37.2093%\" x=\"16.2791%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NP</text></svg><svg width=\"15.625%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">the</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"7.8125%\" y1=\"20px\" y2=\"48px\" /><svg width=\"28.125%\" x=\"15.625%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">current</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"29.6875%\" y1=\"20px\" y2=\"48px\" /><svg width=\"28.125%\" x=\"43.75%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">account</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"57.8125%\" y1=\"20px\" y2=\"48px\" /><svg width=\"28.125%\" x=\"71.875%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">deficit</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"85.9375%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"34.8837%\" y1=\"20px\" y2=\"48px\" /><svg width=\"6.97674%\" x=\"53.4884%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">will</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">MD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"56.9767%\" y1=\"20px\" y2=\"48px\" /><svg width=\"9.30233%\" x=\"60.4651%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">narrow</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VB</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"65.1163%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.65116%\" x=\"69.7674%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">to</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">TO</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"72.093%\" y1=\"20px\" y2=\"48px\" /><svg width=\"6.97674%\" x=\"74.4186%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">only</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">RB</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"77.907%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.65116%\" x=\"81.3953%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">8</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"83.7209%\" y1=\"20px\" y2=\"48px\" /><svg width=\"10.4651%\" x=\"86.0465%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">billion</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"91.2791%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.48837%\" x=\"96.5116%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"98.2558%\" y1=\"20px\" y2=\"48px\" /></svg>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Write a Python function using NLTK that takes a sentence as input and returns all verb phrases (VP) present in the sentence."
      ],
      "metadata": {
        "id": "TOAjx6EcCDVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from nltk.chunk import RegexpParser\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def extract_verb_phrases(sentence):\n",
        "    tokens = word_tokenize(sentence)\n",
        "    pos_tags = pos_tag(tokens)\n",
        "\n",
        "    # Improved grammar for verb phrases\n",
        "    grammar = r\"\"\"\n",
        "        VP: {<MD>?<VB.*>+<NP|PP|CLAUSE>*}\n",
        "    \"\"\"\n",
        "\n",
        "    chunk_parser = RegexpParser(grammar)\n",
        "    chunked_tree = chunk_parser.parse(pos_tags)\n",
        "\n",
        "    verb_phrases = []\n",
        "    for subtree in chunked_tree.subtrees():\n",
        "        if subtree.label() == 'VP':\n",
        "            verb_phrases.append(\" \".join(word for word, tag in subtree.leaves()))\n",
        "\n",
        "    return verb_phrases\n",
        "\n",
        "sentence = \"She is writing a research paper on artificial intelligence.\"\n",
        "print(extract_verb_phrases(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFMdiVujB6FI",
        "outputId": "ad7f2e07-da42-44db-f66e-1f6a9767b1c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['is writing']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write a Python program using NLTK to perform named entity recognition (NER) on a paragraph containing multiple sentences. Display the extracted named entities."
      ],
      "metadata": {
        "id": "E4JT5nhOCbV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = \"\"\"Google was founded by Larry Page and Sergey Brin while they were Ph.D. students at Stanford University.\n",
        "               The company was incorporated in California.\"\"\"\n",
        "\n",
        "tokens = word_tokenize(paragraph)\n",
        "pos_tags = pos_tag(tokens)\n",
        "named_entities = ne_chunk(pos_tags)\n",
        "\n",
        "print(named_entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgYULOHuCfxo",
        "outputId": "02a07a81-064a-4903-81e4-e7474f7b132c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PERSON Google/NNP)\n",
            "  was/VBD\n",
            "  founded/VBN\n",
            "  by/IN\n",
            "  (PERSON Larry/NNP Page/NNP)\n",
            "  and/CC\n",
            "  (PERSON Sergey/NNP Brin/NNP)\n",
            "  while/IN\n",
            "  they/PRP\n",
            "  were/VBD\n",
            "  Ph.D./NNP\n",
            "  students/NNS\n",
            "  at/IN\n",
            "  (ORGANIZATION Stanford/NNP University/NNP)\n",
            "  ./.\n",
            "  The/DT\n",
            "  company/NN\n",
            "  was/VBD\n",
            "  incorporated/VBN\n",
            "  in/IN\n",
            "  (GPE California/NNP)\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to count the number of named entities of type GPE (Geopolitical Entity) in a given text."
      ],
      "metadata": {
        "id": "SOCAralLCp5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_gpe_entities(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    named_entities = ne_chunk(pos_tags)\n",
        "\n",
        "    count = sum(1 for subtree in named_entities if hasattr(subtree, 'label') and subtree.label() == 'GPE')\n",
        "    return count\n",
        "\n",
        "text = \"Germany and France are planning a summit in Berlin.\"\n",
        "print(count_gpe_entities(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVkdnPKaCqSY",
        "outputId": "9dbda0df-cfde-49f4-f055-cfd4e5d7475f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python program to extract all organization names from a given text using NLTK's Named Entity Recognition (NER)."
      ],
      "metadata": {
        "id": "6PSBj1B6Cwp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load English NLP model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define a list of words to exclude (common misclassified words)\n",
        "EXCLUDE_ORGS = {\"AI\", \"ML\", \"NLP\"}\n",
        "\n",
        "def extract_organizations_spacy(text):\n",
        "    doc = nlp(text)\n",
        "    return [ent.text for ent in doc.ents if ent.label_ == \"ORG\" and ent.text not in EXCLUDE_ORGS]\n",
        "\n",
        "text = \"Microsoft and Google are competing in the AI space.\"\n",
        "print(\"Extracted Organizations:\", extract_organizations_spacy(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNWwqduqCwEY",
        "outputId": "8ef5c3ac-08b9-416b-ae1f-88c268dfe9cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Organizations: ['Microsoft', 'Google']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python program using NLTK to extract proper nouns (NNP) from a given sentence."
      ],
      "metadata": {
        "id": "WEjzAN8_EEUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_proper_nouns(sentence):\n",
        "    tokens = word_tokenize(sentence)\n",
        "    pos_tags = pos_tag(tokens)\n",
        "\n",
        "    proper_nouns = [word for word, tag in pos_tags if tag == 'NNP']\n",
        "    return proper_nouns\n",
        "\n",
        "sentence = \"John and Mary went to New York City.\"\n",
        "print(extract_proper_nouns(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byOhwBSxEEk4",
        "outputId": "3c59ef2f-532b-45d8-f7ca-7441a4575a6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['John', 'Mary', 'New', 'York', 'City']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Write a Python program to extract noun phrases from a given text using a custom chunking grammar."
      ],
      "metadata": {
        "id": "rMd0f3x_ELUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grammar = \"NP: {<DT>?<JJ>*<NN>+}\"\n",
        "\n",
        "chunk_parser = RegexpParser(grammar)\n",
        "sentence = \"The large brown dog barked at the stranger.\"\n",
        "tokens = word_tokenize(sentence)\n",
        "pos_tags = pos_tag(tokens)\n",
        "\n",
        "chunked_tree = chunk_parser.parse(pos_tags)\n",
        "print(chunked_tree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOu-1AxCELmA",
        "outputId": "d24d0649-9f9c-48c4-8847-bffc35efa32c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (NP The/DT large/JJ brown/JJ dog/NN)\n",
            "  barked/VBD\n",
            "  at/IN\n",
            "  (NP the/DT stranger/NN)\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Write a Python program to extract verb phrases (VP) from a given sentence using a custom chunking grammar."
      ],
      "metadata": {
        "id": "52-55TUUFDew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, RegexpParser\n",
        "\n",
        "# Define proper grammar\n",
        "grammar = r\"\"\"\n",
        "  VP: {<MD>?<VB.*><RB.*>?<VB.*>?}  # Modal verb (optional), main verb, adverb (optional), another verb (optional)\n",
        "\"\"\"\n",
        "\n",
        "chunk_parser = RegexpParser(grammar)\n",
        "\n",
        "# Input sentence\n",
        "sentence = \"The dog is barking at the stranger.\"\n",
        "\n",
        "# Tokenize and tag\n",
        "tokens = word_tokenize(sentence)\n",
        "pos_tags = pos_tag(tokens)\n",
        "\n",
        "# Apply chunking\n",
        "chunked_tree = chunk_parser.parse(pos_tags)\n",
        "\n",
        "# Extract and print verb phrases\n",
        "for subtree in chunked_tree.subtrees():\n",
        "    if subtree.label() == 'VP':\n",
        "        print(\" \".join(word for word, tag in subtree.leaves()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPiDz8GOFFTg",
        "outputId": "1d69ba0b-ccb7-42c3-92fe-242fe47533a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "is barking\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Write a Python program that extracts all named entities and classifies them into their respective categories (PERSON, ORGANIZATION, GPE, etc.)."
      ],
      "metadata": {
        "id": "gvNX8jjLFcPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load English NLP model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def extract_named_entities_with_labels(text):\n",
        "    doc = nlp(text)\n",
        "    return [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "text = \"Elon Musk is the CEO of Tesla and SpaceX, and he lives in California.\"\n",
        "print(extract_named_entities_with_labels(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjNc8SkjFgYY",
        "outputId": "4ee2054f-a6ab-41b4-be97-0d262107668c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Elon Musk', 'PERSON'), ('Tesla', 'ORG'), ('California', 'GPE')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Write a Python program to visualize named entities using ne_chunk from NLTK."
      ],
      "metadata": {
        "id": "zT2zqa1cHPPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install svgling\n",
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "from IPython.display import display\n",
        "import svgling\n",
        "\n",
        "sentence = \"Barack Obama was the 44th President of the United States.\"\n",
        "tokens = word_tokenize(sentence)\n",
        "pos_tags = pos_tag(tokens)\n",
        "named_entities = ne_chunk(pos_tags)\n",
        "\n",
        "# Use display instead of draw:\n",
        "display(named_entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "id": "qkooP4VzHPiM",
        "outputId": "c681a501-37ac-4686-f624-e0260071a0cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tree('S', [Tree('PERSON', [('Barack', 'NNP')]), Tree('PERSON', [('Obama', 'NNP')]), ('was', 'VBD'), ('the', 'DT'), ('44th', 'JJ'), ('President', 'NNP'), ('of', 'IN'), ('the', 'DT'), Tree('GPE', [('United', 'NNP'), ('States', 'NNPS')]), ('.', '.')])"
            ],
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px\" version=\"1.1\" viewBox=\"0,0,568.0,168.0\" width=\"568px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"11.2676%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PERSON</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Barack</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"5.6338%\" y1=\"20px\" y2=\"48px\" /><svg width=\"11.2676%\" x=\"11.2676%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PERSON</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Obama</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"16.9014%\" y1=\"20px\" y2=\"48px\" /><svg width=\"7.04225%\" x=\"22.5352%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">was</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.0563%\" y1=\"20px\" y2=\"48px\" /><svg width=\"7.04225%\" x=\"29.5775%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">the</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"33.0986%\" y1=\"20px\" y2=\"48px\" /><svg width=\"8.4507%\" x=\"36.6197%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">44th</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"40.8451%\" y1=\"20px\" y2=\"48px\" /><svg width=\"15.493%\" x=\"45.0704%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">President</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"52.8169%\" y1=\"20px\" y2=\"48px\" /><svg width=\"5.6338%\" x=\"60.5634%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">of</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"63.3803%\" y1=\"20px\" y2=\"48px\" /><svg width=\"7.04225%\" x=\"66.1972%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">the</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"69.7183%\" y1=\"20px\" y2=\"48px\" /><svg width=\"22.5352%\" x=\"73.2394%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">GPE</text></svg><svg width=\"50%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">United</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"25%\" y1=\"20px\" y2=\"48px\" /><svg width=\"50%\" x=\"50%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">States</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNPS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"75%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"84.507%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.22535%\" x=\"95.7746%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"97.8873%\" y1=\"20px\" y2=\"48px\" /></svg>"
          },
          "metadata": {}
        }
      ]
    }
  ]
}